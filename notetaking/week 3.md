builder pattern and design of rate limiter, along with design of search engine.

design of search engine can be done in different ways including 
having a key value mapping storedin some hashmap or a database using which we can query the frequence of a word however this way of using a relational table to develope a search engine might sound simple but in practice would be slower because of fetching everytime from the database is not a ideal scenario, we should know that this can be optimized usin gtrie datastructiure, where we simply do store one character in a node, and each noode has 26 possiblle children this way we can store the words in a manner in whch we can identify the prfixes of the words, this is much more efficient, however we should want to be storing the count of the word which has been searched once we have that we can either do two things, get the prefix if first 3 words, now we could traverse the children and find the list of the endpoints along with their frequence, we could sort this list to return the top 5 most popular search results, however this woudl be ordre of prefix+ order of child + childlogchild thus to imporvie this we could store the frequency at each level of the trie, we could update the trie once every week so that the updated search results, are returned in the output.

in rag we conver the query in to embeddings, and do a vector search where we find the k nearest vector embeddings, we combine these k embedding chunks to form a context block which is passed along as context along with the whole original query , basically acting like a cheatcode for the llm to produce the output with the context in hand, it can be useful when we do not want to provide our data to external api's like open ai, and using limited context for our prompts.

there are multiple tools which can be used for determining the performance of any application one such tool is visual vm which can make use of the jdbc query performance along with cpu usage to give a verdict on the performance of the system, and thus is very useful

we can make use of intellisell connector for testing soap request that are commonly used.
ideally use one event per query.

gemini cli is a useful tool which if used correctly can have the context of the entire application, and be used to summarize entire text if required, 
not just this it is also capable to perform actions, if we compare the main 3 types of ai applicaiton one is rag: which is basically providing context to the llm to provide better solutions to the problem s, we create a vector database out of our documents, and then when the user prompts we fetch n most relevant embeddings from the vector database and then we use this to 



the way to use these agents can be optimized according to the usage required, you can frme a rough draft of the requiredments and ask them to provide you with the required prompt ask them to be experts in one of the fields, and provide more accurate outputs, the outputs generated by tthese do need to be reviewed , so if you wan the output to be accurate, then you need to ensure that you yourself are reviewing al lthe changes otherwise , they tend to not listen to the feedback , this is especially the case with gemini, which has the tendency to just provide a yes, or yes option lol which means that it is not very good at listening to feedback , so when it comes to automation of task then they do perfrom well especially if the task is tedius but straingforward, eg generting the readme files of a project and pushing the changes to github, however when th task given is not so straight forward they do tend to take more time and struggel however if we get our hands on th paid model like claude they are supposed to be way better thean normal free model and provide better outputs according to the requirement speifications. however currently the model especially ones which are provided by anthropic are expensive they have 3 differnet tiers which include a pay per use model, which i am assuming would be expensive, a limited pro model which asks for 20 dollars a month and another model which asks for 100$per month this provides proper limits , and have sufficient calss, ensuring that any big project can be built. This increases the speed to market which is everything these days, and can become competetive advantage if the benfits are made use of carefully.
however we do need to guide them well and need to behave like their product managers, the more accurate we are in providing queries, the better it is however it does mean that now instead of writing code , we are spending time in optimizing the prompts, it like having a junior dev along with you and has it's tradeoff, the assistant will do the work but wil require a lot of hand holding and it is upto the user to decide if this is worth their time and effort. however i am predicting that over the course of the next 3-4 yhears we would bre having model strong enough that would be able to take decisions on their own and would not require a lot of handholding, our task then would be providing the required context and clear requirements. 